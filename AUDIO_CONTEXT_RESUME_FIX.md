# Audio Context Resume Fix: Multiple Resume Attempts

## Issue After Previous Fix

Muted track detection worked, but audio context resume was **hanging**:

```
ðŸŒ [WebRTC-JS] âš ï¸ Track 0 created MUTED - waiting for unmute event  âœ…
ðŸŒ [WebRTC-JS] ðŸ”§ Audio context state: suspended  âœ…
ðŸŒ [WebRTC-JS] ðŸ”§ Resuming audio context...  âœ…
[... NO SUCCESS/FAILURE LOG ...]  âŒ
[... NO UNMUTE EVENT ...]  âŒ
```

### Root Cause
`audioContext.resume()` was called but the Promise **never resolved or rejected**. This means:
1. iOS is **blocking** the resume attempt
2. Resume was attempted too late (after call already connected)
3. CallKit audio session activation wasn't triggering proper audio context state

## Solution

### Try to Resume Audio Context at Multiple Strategic Points

Instead of only trying once when muted track is detected, now we attempt resume:

#### 1. **Right After getUserMedia() Success**
**Location**: `initializeLocalStream()` function

```javascript
isAudioInitialized = true;
applyMuteStateToStream('local_stream_ready');

// CRITICAL: Try to resume audio context immediately after getting stream
if (audioContext && audioContext.state === 'suspended') {
    console.log('ðŸ”§ [initializeLocalStream] Audio context suspended, resuming NOW...');
    if (typeof Android !== 'undefined' && Android.logToNative) {
        Android.logToNative('ðŸ”§ [WebRTC] Audio context suspended after getUserMedia, resuming...');
    }
    audioContext.resume().then(() => {
        Android.logToNative('âœ…âœ…âœ… [WebRTC] Audio context RESUMED successfully!');
    }).catch(err => {
        Android.logToNative('âŒ [WebRTC] Audio context resume failed: ' + err.message);
    });
}
```

**Why**: iOS might allow resume right after `getUserMedia()` succeeds since user just granted mic permission.

#### 2. **In peer.on('open') After Stream Creation**
**Location**: `peer.on('open')` handler

```javascript
if (typeof Android !== 'undefined' && Android.logToNative) {
    Android.logToNative('âœ…âœ…âœ… [WebRTC] getUserMedia() SUCCESS in peer.on(open)');
    
    // CRITICAL: Try to resume audio context right after stream creation
    if (audioContext) {
        Android.logToNative(`ðŸ”§ [WebRTC] Audio context state in peer.on(open): ${audioContext.state}`);
        if (audioContext.state === 'suspended') {
            Android.logToNative('ðŸ”§ [WebRTC] Attempting to resume audio context in peer.on(open)...');
            audioContext.resume().then(() => {
                Android.logToNative('âœ…âœ…âœ… [WebRTC] Audio context RESUMED in peer.on(open)!');
            }).catch(err => {
                Android.logToNative('âŒ [WebRTC] Audio context resume failed in peer.on(open): ' + err.message);
            });
        } else {
            Android.logToNative(`âœ… [WebRTC] Audio context already ${audioContext.state} - no resume needed`);
        }
    }
}
```

**Why**: When PeerJS connects, we have both user interaction (accepting call) and active audio context from CallKit.

#### 3. **When Muted Track Detected (Existing)**
**Location**: `markConnectedIfNeeded()` function (connection diagnostics)

**Enhanced** with timeout detection:

```javascript
if (audioContext.state === 'suspended') {
    Android.logToNative(`ðŸ”§ [WebRTC] Resuming audio context in muted track recovery...`);
    
    // Set a timeout to detect if resume hangs
    const resumeTimeout = setTimeout(() => {
        Android.logToNative(`â° [WebRTC] Audio context resume taking > 2s - may be blocked by iOS`);
    }, 2000);
    
    audioContext.resume().then(() => {
        clearTimeout(resumeTimeout);
        Android.logToNative(`âœ…âœ…âœ… [WebRTC] Audio context RESUMED in muted track recovery!`);
    }).catch(err => {
        clearTimeout(resumeTimeout);
        Android.logToNative(`âŒ [WebRTC] Failed to resume audio context: ${err.message}`);
    });
} else {
    Android.logToNative(`â„¹ï¸ [WebRTC] Audio context already ${audioContext.state} - no resume needed`);
}
```

**Why**: Last resort if earlier attempts failed. Timeout helps detect if iOS is blocking.

## How It Works

### Timeline of Audio Context Resume Attempts:

```
1. Page loads â†’ Audio context created (state: suspended)
2. CallKit activates â†’ Native audio session active
3. getUserMedia() called â†’ Microphone permission granted
4. âœ… ATTEMPT #1: Resume right after getUserMedia() succeeds
5. PeerJS connects â†’ Peer open event fires
6. âœ… ATTEMPT #2: Resume in peer.on('open') after stream created
7. Android peer calls â†’ Incoming call answered
8. Call connects â†’ Muted track detected
9. âœ… ATTEMPT #3: Resume when muted track detected (with timeout)
```

**Strategy**: Try early and often. iOS might allow resume at any of these points.

## Expected Behavior

### Success Case (Resume Works):
```
ðŸ”§ [WebRTC] Audio context suspended after getUserMedia, resuming...
âœ…âœ…âœ… [WebRTC] Audio context RESUMED successfully!
[... later ...]
âœ… [WebRTC] Track 0: enabled=true, state=live, muted=false
```
â†’ Track never mutes OR unmutes quickly

### Partial Success (Resume Works on 2nd/3rd Attempt):
```
ðŸ”§ [WebRTC] Audio context suspended after getUserMedia, resuming...
[... no response ...]
ðŸ”§ [WebRTC] Attempting to resume audio context in peer.on(open)...
âœ…âœ…âœ… [WebRTC] Audio context RESUMED in peer.on(open)!
âœ…âœ…âœ… [WebRTC] Track 0 UNMUTED - microphone is now producing audio!
```
â†’ Track unmutes after 2nd attempt

### Still Blocked (iOS Blocking Resume):
```
ðŸ”§ [WebRTC] Resuming audio context in muted track recovery...
â° [WebRTC] Audio context resume taking > 2s - may be blocked by iOS
[... track stays muted ...]
```
â†’ Need different approach (see "Next Steps" below)

## Files Modified
- `/Enclosure/VoiceCallAssets/scriptVoice.js`:
  - Added audio context resume attempt right after `getUserMedia()` success
  - Added audio context resume attempt in `peer.on('open')` handler
  - Enhanced muted track recovery with timeout detection
  - Added state logging at each attempt

## Testing Instructions

1. **Clean build** and run on device
2. **Make call from Android**
3. **Accept from iOS foreground**
4. **Watch for new logs**:

### Look for Resume Attempts:
```
ðŸ”§ [WebRTC] Audio context suspended after getUserMedia, resuming...
```

### Look for Success:
```
âœ…âœ…âœ… [WebRTC] Audio context RESUMED successfully!
```
OR
```
âœ…âœ…âœ… [WebRTC] Audio context RESUMED in peer.on(open)!
```
OR
```
âœ…âœ…âœ… [WebRTC] Audio context RESUMED in muted track recovery!
```

### Look for Unmute Event:
```
âœ…âœ…âœ… [WebRTC] Track 0 UNMUTED - microphone is now producing audio!
```

### Check for Timeout (If Resume Hangs):
```
â° [WebRTC] Audio context resume taking > 2s - may be blocked by iOS
```

## Expected Outcome

### Best Case:
Audio context resumes on **1st attempt** (right after getUserMedia), track is never muted, microphone works immediately! âœ…

### Good Case:
Audio context resumes on **2nd attempt** (in peer.on open), track unmutes within 1-2 seconds, microphone works! âœ…

### Acceptable Case:
Audio context resumes on **3rd attempt** (muted track recovery), track unmutes after call connects, microphone works with slight delay! âœ…

### Needs More Work:
All resume attempts **timeout or hang**, track stays muted. Logs will show which attempts were made and where they failed. This will guide next fix.

## Next Steps If Still Blocked

If all resume attempts fail/hang, the logs will reveal:
1. Audio context state at each attempt point
2. Whether promises resolve/reject or hang
3. Whether iOS is blocking all resume attempts

Possible next fixes if this doesn't work:
1. **Force user interaction**: Require tap/touch before allowing audio
2. **Native bridge**: Resume audio context from native iOS code through CallKit
3. **New stream**: Create fresh getUserMedia stream if track stays muted > 3s
4. **Audio element trick**: Play silent audio element to wake iOS audio system
